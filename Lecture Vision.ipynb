{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To See Without My Eyes\n",
    "\n",
    "Welcome to the Python OpenCV workshop.\n",
    "\n",
    "This is an introduction to the theory of Computer Vision and open-source  OpenCV library. Over the course of the next two days, we will show you the differences between human and machine sight, the intricacies of image data, image processing techniques, pixel operations, and show you several code examples of the openCV library.\n",
    "\n",
    "By the end, you'll know the fundamentals of computer vision technologies and understand how to deploy them for your own projects\n",
    "\n",
    "With that, let's get started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we see?\n",
    "\n",
    "Human sight is complex.\n",
    "\n",
    "Our eyes respond to light. Rods and cones receive that light.\n",
    "\n",
    "![Rods and Cones from https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse2.mm.bing.net%2Fth%3Fid%3DOIP.2gyZ_2w51y8qEeCCcIyeJAHaEp%26pid%3DApi&f=1 ](img/rods_and_cones_1.jfif)\n",
    "\n",
    "Light photons hit our eyes, with specific wavelengths causing a response that causes our visual cortex to activate in a particular pattern. Triggering the optical perception of things.\n",
    "\n",
    "But our vision is something much bigger than that.\n",
    "\n",
    "What do you see in this image? Take a few seconds to describe what you see. Write some options down.\n",
    "\n",
    "\n",
    "![What is this? from Zero Escape 999](img/funyaripa.jpg)\n",
    "\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "Seriously! Internalize that image before continuing\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "What did you see in there? It's a strange image to be sure. The underlying source for that image is actually a dog! \n",
    "\n",
    "![The \"Funyarinpa\" from Zero Escape 999](img/Dog.png)\n",
    "\n",
    "Vision is not just what you receive in your eyeballs. It is also what your brain can picture. Your imagination plays a strong role in what an image can become. These top-down concepts are things that all affect your ability to see something. Look at this dog, and then look back above. Now try to see anything but the dog.\n",
    "\n",
    "This is _bias_. It is the psychological force that makes us see things out of images. It is fundamental to how we are able to perceive the world. \n",
    "\n",
    "Our vision is imperfect and subject to cognitive forces. Keep these ideas at the front of your mind as we go forward because computer vision is basically all about replicating bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's in a Digital Image?\n",
    "\n",
    "On the surface, one is inclined to say that the computer inherits none of those biases. A computer works with numbers and abstract data. It is unable to have a top down perception loop.\n",
    "\n",
    "It is in the programming that introduces our influence into the machine. When we tell a computer to process imagery - it is processed in similar terms that a human can understand and process them. Ultimately, a computer cannot see the way a human does. _We make it imitate us, and therefore takes it takes on the biases of our vision_.\n",
    "\n",
    "But without eyes, how can a computer see? Data. Data representing what you might ask?\n",
    "\n",
    "![cone_sensitivity](img/cone_sensitivity.jpg)\n",
    "\n",
    "![density image of cones and rods](img/rod_and_cone_density.gif)\n",
    "These diagrams show us what we know about the physical side of human sight: color perception and detail.\n",
    "\n",
    "Computer imagery data (and thus computer vision) is based aroudn these human perceptual limitations.\n",
    "\n",
    "__Color__ is represented by a 3 channel set of Red, Green, and Blue, often refererred to as RGB.\n",
    "\n",
    "__Detail__ is represented by pixel density or resolution. It is the resolvable level of detail in a computer image.\n",
    "\n",
    "Computer image file types like the jpg the gif and the png contain image data that roughly corresponds to a pixel and color data. File types that contain more, the psd, svg, or ai, contain data that can produce this pixel data to be shown on a screen.\n",
    "\n",
    "Know that even this representation is inaccurate. Our eyes do not work on a seperate RGB scale; they respond to several wavelengths of light, triggering different physiological responses. This doesn't even cover expanded wavelengths which are imperceptible to humans _but not other detectors that a computer might have_. Things like infared or ultraviolet cameras are just some of the ways of getting beyond human image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import cv2 \n",
    "import matplotlib.pyplot as plt  #image plotting\n",
    "%matplotlib inline\n",
    "import os #For interacting with Operating System - we use it for going through files\n",
    "import glob #For file name pattern matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that helps us look at the pixel data\n",
    "def OutputImageAsText(img):\n",
    "    w,h,_ = img.shape #get the image dimensions\n",
    "    for x in range(w):\n",
    "        for y in range(h):\n",
    "            #get the color data at pixel position\n",
    "            print(img[x,y])\n",
    "\n",
    "\n",
    "#Load image in as grayscale for demonstration\n",
    "img = cv2.imread(\"img/not_a_pipe.jpg\")\n",
    "\n",
    "###################\n",
    "#\n",
    "# What's easier to read?\n",
    "#\n",
    "######################\n",
    "\n",
    "\n",
    "#This?\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "# Or This?\n",
    "OutputImageAsText(img) #note the [B,G,R] values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's in this data that we have a belief about what should be codified as baseline of images. Not all images have to adhere to it, but the vast majority does. There is a built-in assumption about what an image's data representation should be. Maybe there is a better way? But recognizing this representation will help you understand OpenCV because OpenCV works completely within this framework of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We are always working with an approximation of images. A computer is unable to understand images as we do, and so we work with the intermediary image data to communicate with the machine.\n",
    "\n",
    "Any computer vision implementation is ultimately bound by what it can respond to as image data. There is no reason that it has to be based on human sight. In fact, the broader field of CV looks at other ways of transforming data into an image representation.\n",
    "\n",
    "This image created by NASA is one such example\n",
    "![blackholdnasa](img/black_hole_image.jpg)\n",
    "\n",
    "Or this image which describes surface's physical texture.\n",
    "![normalmap](img/normal_map_example.png)\n",
    "\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "At the end of the day, it's important to recognize that computer imagery and the \"vision\" it provides is always subject to the treachery of data. It is always representational, and when it comes to computers, even vision is just what we tell it to be.\n",
    "\n",
    "![not a pipe](img/not_a_pipe.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Convolution\n",
    "\n",
    "Now you know how to handle pixels and image data the way a computer might.\n",
    "\n",
    "You understand pixels can be read, they can be data for anything really. They just happen to be used to produce images on our screen. \n",
    "\n",
    "But what if we want to start doing smarter changes to the images itself? To do that, we need to know what a pixel is representing. \n",
    "\n",
    "And for that, we sample multiple pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "\n",
    "To figure out what a pixel should do, we should look to the pixels around it. This way we can aggregate them.\n",
    "\n",
    "This is the foundation of several image effects, like blur.\n",
    "\n",
    "How do we blur an image? Well, we reduce the detail. How do we do that? We take every pixel and make the surrounding pixels contribute to its color. We can do an average to make that.\n",
    "\n",
    "Let's try that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image bluring box blur\n",
    "#Be aware that this block can take a long time to run\n",
    "\n",
    "#Load image in as grayscale for demonstration\n",
    "\n",
    "\n",
    "img = cv2.imread(\"img/not_a_pipe.jpg\")\n",
    "w,h,_ = img.shape #get the image's width and height\n",
    "\n",
    "\n",
    "avg = [None] * w #create a matrix for the averaged image\n",
    "\n",
    "#loop through every pixel on the image\n",
    "for x in range(w):\n",
    "    avg[x] = [None] * h #add a matrix column\n",
    "    for y in range(h):\n",
    "        #read the pixel value\n",
    "        pix = 0\n",
    "        count = 0 \n",
    "        #accumulate an average pixel value\n",
    "        for i in range(-1,2,1): #size 2 box blur\n",
    "            for j in range(-1,2,1):\n",
    "                \n",
    "                if ((x + i) < 0 or (x + i) >= w or (y + j) < 0 or (y + j) >= h):\n",
    "                    pass #handle literal edge cases\n",
    "                else:\n",
    "                    count += 1 #count that we added a pixel\n",
    "                    sample = img[x+i][y+j] #get the pixel sample\n",
    "                    #average the color for a simple grayscale blur\n",
    "                    #luminance would be preferred\n",
    "                    pix += (sample[0] + sample[1] + sample[2])/3\n",
    "        pix /= count #average out the color\n",
    "        pix /= 100 #this is a brightness adjustment\n",
    "        \n",
    "        avg[x][y] = [pix,pix,pix] #set the average pixel\n",
    "\n",
    "#blurred image\n",
    "print('The blurred image, with artifacts')\n",
    "imgplot = plt.imshow(avg)\n",
    "plt.show()\n",
    "\n",
    "#Original for comparison\n",
    "print('And the original for comparison')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels\n",
    "\n",
    "It turns out that the above operation can be simplied into a matrix multiplication of two matricies.\n",
    "\n",
    "![image blur link](https://datacarpentry.org/image-processing/fig/05-blur-demo.gif)\n",
    "https://datacarpentry.org/image-processing/06-blurring/\n",
    "\n",
    "This is a _kernel_. All these convolutions rely on a kernel, a matrix of values that tells the pixel how to weight the pixels around itself. A simple blur kernel might look like this:\n",
    "\n",
    "[1, 1, 1]\\\n",
    "[1, 1, 1]\\\n",
    "[1, 1, 1]\n",
    "\n",
    "The 1s representing the balanced sum that every pixel will contribute. So instead of using our calculated method that goes through pixel by pixel. Using Kernels is how the rest of the effects will work.\n",
    "\n",
    "Lucky for us, OpenCV has an easy way of implementing this.\n",
    "https://docs.opencv.org/master/d4/dbd/tutorial_filter_2d.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Blur with Kernels\n",
    "blurKernel = np.array([[1/9, 1/9, 1/9],\n",
    "                   [1/9, 1/9, 1/9],\n",
    "                   [1/9, 1/9, 1/9]]) #custome kernels need the division built in\n",
    "\n",
    "img = cv2.imread('img/not_a_pipe.jpg')\n",
    "blur = cv2.filter2D(img, -1, blurKernel) #apply Kernel\n",
    "\n",
    "#Show original\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "#Show Blurred\n",
    "imgplot = plt.imshow(blur)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Edge Detection Version\n",
    "edgeKernel = np.array([[-1, -1, -1],\n",
    "                   [-1, 8, -1],\n",
    "                   [-1, -1, -1]])\n",
    "edge = cv2.filter2D(img, -1, edgeKernel)\n",
    "imgplot = plt.imshow(edge)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More than Effects\n",
    "\n",
    "Beyond the blur, what if you had a specific effect that you wanted to achieve? Like say, the detection of edges or the detection of shapes?\n",
    "\n",
    "This one just finds vertical edges\n",
    "[-1, 0, -1]\\\n",
    "[-2, 0, -2]\\\n",
    "[-1, 0, -1]\n",
    "\n",
    "And this one finds horizontal edges\n",
    "[-1, -2, -1]\\\n",
    "[0, 0, 0]\\\n",
    "[-1, -2, -1]\n",
    "\n",
    "We won't go too deeply into the variety of kernels here, but you can read more here https://www.pyimagesearch.com/2016/07/25/convolutions-with-opencv-and-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms of Vision\n",
    "\n",
    "Once you can convert an image into its various forms. You probably can envision using several of them in concert to \"figure out\" an image's qualities.\n",
    "\n",
    "And that is exactly what an algorithm like the Canny Edge detection algorithm is for. https://docs.opencv.org/master/da/d22/tutorial_py_canny.html\n",
    "\n",
    "It is a series of kernels that does the following:\n",
    "1. Noise reduction (blur)\n",
    "2. Performs an edge dection using the sobel operator (edge)\n",
    "3. Does a surpression of noisy pixels again (another blur)\n",
    "4. Does a threshold check to make sure edges are within a boundary. (more complex operator)\n",
    "5. Produces the image\n",
    "\n",
    "In OpenCV, this is turned into a single function Canny(). Yes, it can be that easy. And now you understand every step that goes into it.\n",
    "\n",
    "The algorithm is named for John Canny who invented it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#canny example\n",
    "\n",
    "img = cv2.imread('img/not_a_pipe.jpg')\n",
    "edges = cv2.Canny(img,100,200)\n",
    "plt.imshow(edges)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Limits of Detection\n",
    "\n",
    "Remember that we are always trying to transform images into concepts that we can perceive as humans. So even complex algorithmic techniques are susceptible to exceptions. They have blindspots, so to speak.\n",
    "\n",
    "You will encounter plenty if you continue down the route of OpenCV\n",
    "https://dsp.stackexchange.com/questions/1714/best-way-of-segmenting-veins-in-leaves\n",
    "\n",
    "A plain leaf, how do we figure out the veins?\n",
    "![leaf plain](img/leaf_plain.png)\n",
    "\n",
    "The leaf with a Canny edge detector.\n",
    "![leaf plain](img/leaf_canny.jpg)\n",
    "\n",
    "The leaf with a naive sobel filter\n",
    "![leaf plain](img/leaf_sobel.jpg)\n",
    "\n",
    "Even our dog is difficult to produce clear edges with a 3x3 Sobel operator. Maybe a 5x5 would fare better? It is hard to say.\n",
    "\n",
    "Just know that the kernels and algorithms that you use from here are based on a human's perspective on what marks an edge and what marks an object. It is always subject to limitations, just as their eyes are limited to rods and cones.\n",
    "\n",
    "Here is where you must begin to make your own decisions about what makes a good algorithm for detection. On the road of computer vision, never forget that you decide what will be computed and what will be seen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/\n",
    "import sys\n",
    "#!{sys.executable} -m pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
